# src/rag/etl.py
import os
import tempfile
from pathlib import Path
from typing import List, Optional

from streamlit.runtime.uploaded_file_manager import UploadedFile

from langchain_community.document_loaders import (
    TextLoader, 
    PyPDFLoader, 
    Docx2txtLoader, 
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config.settings import settings
from src.utils.logger import setup_logger

logger = setup_logger("RAG_ETL")

# é’ˆå¯¹ä¸­æ–‡æ–‡æ¡£çš„åˆ†éš”ç¬¦ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
CHINESE_SEPARATORS = [
    "\n\n",    # æ®µè½åˆ†éš”
    "\n",      # è¡Œåˆ†éš”
    "ã€‚",      # ä¸­æ–‡å¥å·
    "ï¼",      # ä¸­æ–‡æ„Ÿå¹å·
    "ï¼Ÿ",      # ä¸­æ–‡é—®å·
    "ï¼›",      # ä¸­æ–‡åˆ†å·
    "ï¼Œ",      # ä¸­æ–‡é€—å·
    "ï¼š",      # ä¸­æ–‡å†’å·
    "ã€",      # ä¸­æ–‡å³å¼•å·
    "ã€",      # ä¸­æ–‡å³è§’å¼•å·
    "ã€",      # ä¸­æ–‡å·¦åŒå¼•å·
    "ã€",      # ä¸­æ–‡å³åŒå¼•å·
    "(", ")",  # è‹±æ–‡æ‹¬å·
    "ï¼ˆ", "ï¼‰", # ä¸­æ–‡æ‹¬å·
    " ",       # ç©ºæ ¼
    ""         # æœ€åæŒ‰å­—ç¬¦åˆ‡åˆ†
]

# é’ˆå¯¹ Markdown æ–‡æ¡£çš„åˆ†éš”ç¬¦
MARKDOWN_SEPARATORS = [
    "\n\n",    # æ®µè½åˆ†éš”
    "\n",      # è¡Œåˆ†éš”
    "```",     # ä»£ç å—
    "## ",     # äºŒçº§æ ‡é¢˜
    "### ",    # ä¸‰çº§æ ‡é¢˜
    "#### ",   # å››çº§æ ‡é¢˜
    "- ",      # åˆ—è¡¨é¡¹
    "* ",      # åˆ—è¡¨é¡¹
    "ã€‚",      # ä¸­æ–‡å¥å·
    "ï¼",      # ä¸­æ–‡æ„Ÿå¹å·
    "ï¼Ÿ",      # ä¸­æ–‡é—®å·
    "ï¼›",      # ä¸­æ–‡åˆ†å·
    "ï¼Œ",      # ä¸­æ–‡é€—å·
    " ",       # ç©ºæ ¼
    ""         # æœ€åæŒ‰å­—ç¬¦åˆ‡åˆ†
]


class ContentProcessor:
    def __init__(self):
        self.chunk_size = settings.CHUNK_SIZE
        self.chunk_overlap = settings.CHUNK_OVERLAP

    def load_uploaded_files(self, uploaded_files: List[UploadedFile]) -> List:
        """
        ç›´æ¥å¤„ç†å†…å­˜ä¸­çš„æ–‡ä»¶å¯¹è±¡ï¼Œä¸æŒä¹…åŒ–ä¿å­˜åˆ°ç£ç›˜ã€‚
        ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æŠ€æœ¯é€‚é… LangChain Loaderã€‚
        """
        documents = []
        
        for up_file in uploaded_files:
            tmp_path = None
            try:
                # 1. åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                suffix = Path(up_file.name).suffix
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                    tmp_file.write(up_file.getvalue())
                    tmp_path = tmp_file.name
                
                # 2. é€‰æ‹©åŠ è½½å™¨å¹¶è¯»å–
                logger.info(f"ğŸ“„ æ­£åœ¨å¤„ç†: {up_file.name}")
                loader = self._select_loader(tmp_path, up_file.name)
                
                if loader:
                    docs = loader.load()
                    
                    # 3. å…ƒæ•°æ®ä¿®å¤ï¼šå°† source æ”¹å›åŸå§‹æ–‡ä»¶å
                    for doc in docs:
                        doc.metadata["source"] = up_file.name
                        # è®°å½•æ–‡ä»¶ç±»å‹ï¼Œä¾¿äºåç»­é€‰æ‹©åˆ‡åˆ†ç­–ç•¥
                        doc.metadata["file_type"] = suffix.lower()
                        
                    documents.extend(docs)
                    logger.info(f"   âœ… æˆåŠŸè§£æ {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {up_file.name} å¤±è´¥: {e}")
            finally:
                # 4. ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«æ¸…ç†
                if tmp_path and os.path.exists(tmp_path):
                    try:
                        os.remove(tmp_path)
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        logger.info(f"âœ… åŠ è½½å®Œæˆ: å…±è§£æ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return documents

    def _select_loader(self, file_path: str, original_name: str) -> Optional[object]:
        """æ ¹æ®æ–‡ä»¶åç¼€é€‰æ‹©åŠ è½½å™¨"""
        suffix = Path(original_name).suffix.lower()
        
        try:
            if suffix == ".txt":
                # å°è¯•å¤šç§ç¼–ç ï¼Œå¢å¼ºå®¹é”™æ€§
                return self._create_text_loader_with_fallback(file_path)
            elif suffix == ".md":
                return UnstructuredMarkdownLoader(file_path)
            elif suffix == ".pdf":
                return PyPDFLoader(file_path)
            elif suffix == ".docx":
                return Docx2txtLoader(file_path)
            elif suffix in [".py", ".js", ".java", ".c", ".cpp", ".ts", ".go", ".rs"]:
                # ä»£ç æ–‡ä»¶ä½¿ç”¨ TextLoader
                return TextLoader(file_path, encoding="utf-8")
            else:
                logger.warning(f"âš ï¸ æš‚ä¸æ”¯æŒæ ¼å¼: {suffix}")
                return None
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåŠ è½½å™¨å¤±è´¥ ({suffix}): {e}")
            return None

    def _create_text_loader_with_fallback(self, file_path: str) -> TextLoader:
        """åˆ›å»ºå¸¦æœ‰ç¼–ç å®¹é”™çš„ TextLoader"""
        # ä¼˜å…ˆå°è¯• UTF-8
        try:
            loader = TextLoader(file_path, encoding="utf-8")
            # å°è¯•è¯»å–ä¸€å°æ®µæ¥éªŒè¯ç¼–ç 
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read(1024)
            return loader
        except UnicodeDecodeError:
            # å›é€€åˆ° GBKï¼ˆå¸¸è§äº Windows ä¸­æ–‡ç¯å¢ƒï¼‰
            logger.info(f"   ğŸ“ æ£€æµ‹åˆ°é UTF-8 ç¼–ç ï¼Œå°è¯• GBK...")
            return TextLoader(file_path, encoding="gbk", errors="ignore")

    def split_documents(self, documents: List) -> List:
        """
        æ™ºèƒ½åˆ‡åˆ†æ–‡æ¡£ï¼š
        - æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©ä¸åŒçš„åˆ†éš”ç¬¦ç­–ç•¥
        - éä»£ç æ–‡ä»¶ä½¿ç”¨ä¼˜åŒ–çš„ä¸­æ–‡åˆ†éš”ç¬¦
        """
        if not documents:
            return []
        
        all_chunks = []
        
        # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„å¤„ç†
        for doc in documents:
            file_type = doc.metadata.get("file_type", "").lower()
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆ†éš”ç¬¦
            if file_type == ".md":
                separators = MARKDOWN_SEPARATORS
            elif file_type in [".py", ".js", ".java", ".c", ".cpp", ".ts", ".go", ".rs"]:
                # ä»£ç æ–‡ä»¶ä¿æŒåŸæœ‰é€»è¾‘ï¼ˆæš‚ä¸ä¼˜åŒ–ï¼‰
                separators = ["\n\nclass ", "\n\ndef ", "\n\nclass ", "\n\ndef ",
                             "\nclass ", "\ndef ", "\n\n", "\n", " ", ""]
            else:
                # txtã€pdfã€docx ç­‰æ™®é€šæ–‡æ¡£ä½¿ç”¨ä¸­æ–‡åˆ†éš”ç¬¦
                separators = CHINESE_SEPARATORS
            
            # åˆ›å»ºåˆ‡åˆ†å™¨
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=separators,
                length_function=len,
                is_separator_regex=False
            )
            
            # åˆ‡åˆ†å•ä¸ªæ–‡æ¡£
            chunks = text_splitter.split_documents([doc])
            all_chunks.extend(chunks)
        
        logger.info(f"âœ… åˆ‡åˆ†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ â†’ {len(all_chunks)} ä¸ªç‰‡æ®µ")
        return all_chunks