#src/rag/generator.py

#é“¾æ¥openaiçš„chatæ¨¡å‹ï¼Œè¿›è¡Œå›ç­”ç”Ÿæˆ
from langchain_openai import ChatOpenAI

#è¯æœ¯æ¨¡æ¿ï¼Œæ„å»ºAIæç¤ºè¯
from langchain_core.prompts import ChatPromptTemplate
# è¾“å‡ºè§£æå™¨ï¼Œå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²
from langchain_core.output_parsers import StrOutputParser

#  RunnablePassthrough ç”¨äºå°†è¾“å…¥ç›´æ¥ä¼ é€’ç»™è¾“å‡ºï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
from langchain_core.runnables import RunnablePassthrough

from src.rag.retriever import VectorRetriever
from config.settings import settings
from src.utils.logger import setup_logger

logger = setup_logger("RAG_Generator")

class RAGGenerator:
    def __init__(self):
        self.retriever = VectorRetriever()

        self.llm = ChatOpenAI(
            model_name=settings.CHAT_MODEL,
            temperature=0.1,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_BASE_URL

        )
        # --- 3. å®šä¹‰æç¤ºè¯ (Prompt) ---
        # from_template æ–¹æ³•å…è®¸æˆ‘ä»¬ç”¨ {variable} çš„æ ¼å¼æŒ–å‘ï¼Œåé¢å†å¡«å¡«ç©ºé¢˜ã€‚
        self.prompt_template = ChatPromptTemplate.from_template("""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘:
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘:
{question}

ã€å›ç­”è§„åˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘:
1. ä½ å¿…é¡»åŸºäºä¸Šé¢çš„ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ï¼Œè¿™æ˜¯ä½ å”¯ä¸€çš„ä¿¡æ¯æ¥æº
2. ç¦æ­¢è¯´"æœªæ‰¾åˆ°"ã€"æ— æ³•å›ç­”"ã€"æ²¡æœ‰ç›¸å…³ä¿¡æ¯"ç­‰æ‹’ç»æ€§è¯­å¥
3. å¦‚æœä¸Šä¸‹æ–‡ä¸­ç¡®å®æœ‰å†…å®¹ï¼Œä½ å°±å¿…é¡»å¯¹è¿™äº›å†…å®¹è¿›è¡Œæ€»ç»“ã€æ•´ç†æˆ–è§£é‡Š
4. å³ä½¿ä¸Šä¸‹æ–‡ä¸é—®é¢˜ä¸æ˜¯100%åŒ¹é…ï¼Œä¹Ÿè¦å°½åŠ›ä»ä¸Šä¸‹æ–‡ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯å›ç­”
5. å¦‚æœç”¨æˆ·é—®çš„æ˜¯æŸä¸ªæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„ã€æ¥æºã€‘ä¿¡æ¯ï¼Œæ‰¾åˆ°åŒ¹é…çš„å†…å®¹è¿›è¡Œæ•´ç†

ç°åœ¨è¯·åŸºäºä¸Šä¸‹æ–‡å†…å®¹ç›´æ¥ç»™å‡ºå›ç­”:""")
        
    def _format_docs(self, docs):
        """
        æ•°æ®æ¸…æ´—
        docsæ˜¯ä¸€ä¸ªåˆ—è¡¨
        å°†æ–‡æ¡£åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        """
        formatted = []
        for i, doc in enumerate(docs):
            # è·å–æ¥æºä¿¡æ¯
            source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
            formatted.append(f"ã€æ–‡æ¡£{i+1} æ¥æº: {source}ã€‘\n{doc.page_content}")
        return "\n\n".join(formatted)

    def get_answer(self, question: str, session_id=None, project_id="default"):
        """
        ç”Ÿæˆå›ç­”
        question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        """
        logger.info(f"ğŸ¤– æ”¶åˆ°é—®é¢˜: {question} (Session: {session_id})")

        # 1. åˆå§‹åŒ–æ£€ç´¢å™¨
        docs = self.retriever.query(question, project_id=project_id, top_k=3)
        # å…œåº•é€»è¾‘ï¼šå¦‚æœæ•°æ®åº“æ˜¯ç©ºçš„ï¼Œæˆ–è€…å•¥ä¹Ÿæ²¡æŸ¥åˆ°ï¼Œç›´æ¥è¿”å›
        if not docs:
            logger.warning("âš ï¸ çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä»»ä½•ç›¸å…³æ–‡æ¡£ã€‚")
            return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚"
        else:
            logger.info(f"æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        # --- ğŸ”´ æ–°å¢è°ƒè¯•æ‰“å° ---
        # è¿™ä¸€æ®µå¯ä»¥è®©ä½ åœ¨æ§åˆ¶å°çœ‹åˆ°æ£€ç´¢åˆ°çš„å…·ä½“å†…å®¹ï¼Œæ’æŸ¥ä¸ºä»€ä¹ˆ AI è§‰å¾—æ²¡ç­”æ¡ˆ
        print("\n" + "="*20 + " [è°ƒè¯•] æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ " + "="*20)
        for i, (doc, score) in enumerate(docs):
            print(f"ğŸ“„ ç‰‡æ®µ {i+1} (åŒ¹é…åˆ† {score:.2f}):\n{doc.page_content.strip()[:100]}...") # åªçœ‹å‰100å­—
        print("="*60 + "\n")
        # ---------------------
        # å°†æŸ¥åˆ°çš„å¯¹è±¡åˆ—è¡¨ (docs) é‡Œçš„åˆ†æ•°å»æ‰ï¼Œåªä¿ç•™æ–‡æ¡£å¯¹è±¡ï¼Œç„¶åæ¸…æ´—æˆå­—ç¬¦ä¸²
        # docs é‡Œçš„ç»“æ„æ˜¯ [(Document, score), (Document, score)...]
        # åˆ—è¡¨æ¨å¯¼å¼ [doc for doc, score in docs] å–å‡ºäº†å…¶ä¸­çš„ Document
        # ç„¶åç”¨ _format_docs æ–¹æ³•å°†å®ƒä»¬æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        context = self._format_docs([doc for doc, score in docs])

        logger.info(f"æ£€ç´¢ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

        # 2. æ„å»ºé“¾
        # ç›´æ¥ä½¿ç”¨ prompt | llm | parser çš„ç®€å•é“¾ç»“æ„
        # invoke æ—¶ç›´æ¥ä¼ å…¥åŒ…å« context å’Œ question çš„å­—å…¸
        rag_chain = self.prompt_template | self.llm | StrOutputParser()

        try:
            logger.info("è°ƒç”¨ LLM ç”Ÿæˆå›ç­”ä¸­...")
            logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡å†…å®¹é¢„è§ˆ: {context[:200]}...")
            # --- æ­¥éª¤ C: æ‰§è¡Œ (Invoke) ---
            # invoke æ˜¯å¯åŠ¨é”®ã€‚
            # æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸é‡Œçš„ key (context, question) å¿…é¡»å¯¹åº” self.prompt é‡ŒæŒ–çš„é‚£ä¸ªå‘ {context}, {question}ã€‚
            answer = rag_chain.invoke({"context": context, "question": question})
            logger.info(f"âœ… LLM ç”Ÿæˆçš„å›ç­”: {answer}")
            return answer
        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å‡ºé”™: {e}")
            return "ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•ã€‚"