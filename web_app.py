# web_app.py
import streamlit as st
import os
import shutil
from langchain_core.messages import HumanMessage, AIMessage

# å¼•å…¥æˆ‘ä»¬çš„æ ¸å¿ƒé€»è¾‘
from src.agent.graph import app as agent_app
from src.rag.etl import ContentProcessor
from src.rag.vectorstore import VectorDBManager
from config.settings import settings

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ä¼ä¸šçº§ RAG æ™ºèƒ½åŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– ä¼ä¸šçº§ RAG + Multi-Agent åä½œç³»ç»Ÿ")

# --- ä¾§è¾¹æ ï¼šçŸ¥è¯†åº“ç®¡ç† ---
with st.sidebar:
    st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
    
    # 1. æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£ (TXT, MD)", 
        type=["txt", "md"], 
        accept_multiple_files=True
    )
    
    # 2. ä¸Šä¼ ä¸é‡å»ºæŒ‰é’®
    if st.button("ğŸš€ æ›´æ–°çŸ¥è¯†åº“"):
        if not uploaded_files:
            st.warning("è¯·å…ˆé€‰æ‹©æ–‡ä»¶ï¼")
        else:
            status_text = st.empty()
            status_text.info("æ­£åœ¨å¤„ç†æ–‡ä»¶...")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not settings.DATA_DIR.exists():
                settings.DATA_DIR.mkdir(parents=True)
            
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ° data/raw
            saved_count = 0
            for uploaded_file in uploaded_files:
                file_path = settings.DATA_DIR / uploaded_file.name
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                saved_count += 1
            
            status_text.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ ({saved_count} ä¸ª)ï¼Œå¼€å§‹æ„å»ºç´¢å¼•...")
            
            # --- è°ƒç”¨åç«¯é€»è¾‘ (ETL + å…¥åº“) ---
            try:
                # 1. åŠ è½½ä¸åˆ‡åˆ†
                processor = ContentProcessor()
                docs = processor.load_documents()
                chunks = processor.split_documents(docs)
                
                # 2. å‘é‡åŒ–å…¥åº“ (ä½¿ç”¨ append æ¨¡å¼)
                vector_manager = VectorDBManager()
                vector_manager.create_vector_db(chunks, mode="append")
                
                status_text.success("âœ… çŸ¥è¯†åº“æ›´æ–°å®Œæˆï¼Agent å·²è¯»å–æœ€æ–°æ–‡æ¡£ã€‚")
            except Exception as e:
                status_text.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")

    st.divider()
    st.markdown("### è°ƒè¯•ä¿¡æ¯")
    st.info(f"å½“å‰æ¨¡å‹: {settings.CHAT_MODEL}")

# --- ä¸»ç•Œé¢ï¼šèŠå¤©çª—å£ ---

# 1. åˆå§‹åŒ–èŠå¤©å†å² (Session State)
# Streamlit æ¯æ¬¡åˆ·æ–°éƒ½ä¼šé‡ç½®å˜é‡ï¼Œæ‰€ä»¥éœ€è¦ç”¨ session_state è®°ä½èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

# 2. æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    # message æ˜¯ (role, content) çš„å­—å…¸
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 3. å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # A. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    # è®°å½•åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})

    # B. è°ƒç”¨ Agent (æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # æ„é€  LangGraph è¾“å…¥
        inputs = {"messages": [HumanMessage(content=prompt)]}
        
        # å®æ—¶æµå¼è¾“å‡º (Stream)
        # è¿™é‡Œçš„ stream ç¨å¾®å¤æ‚ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬è¦è¿‡æ»¤å‡ºâ€œæœ€ç»ˆå›ç­”â€
        try:
            status_container = st.status("ğŸ¤– Agent æ­£åœ¨æ€è€ƒ...", expanded=True)
            
            for event in agent_app.stream(inputs):
                for node_name, node_output in event.items():
                    # åœ¨æŠ˜å é¢æ¿é‡Œæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                    if node_name == "researcher":
                        status_container.write("ğŸ” ç ”ç©¶å‘˜: æ­£åœ¨åˆ†æéœ€æ±‚...")
                    elif node_name == "tools":
                        status_container.write("ğŸ“š å·¥å…·: æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...")
                    elif node_name == "writer":
                        status_container.write("âœï¸ ä½œå®¶: æ­£åœ¨æ’°å†™å›å¤...")
                        # æ‹¿åˆ°æœ€ç»ˆç»“æœ
                        final_msg = node_output["messages"][-1]
                        full_response = final_msg.content
            
            status_container.update(label="âœ… å›ç­”å®Œæˆ", state="complete", expanded=False)
            
            # æ˜¾ç¤ºæœ€ç»ˆå›ç­”
            message_placeholder.markdown(full_response)
            
            # è®°å½• AI å›ç­”åˆ°å†å²
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")